{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c951b3d-2a62-4a6b-9f06-54c4300bf60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/anaconda3/envs/svejk_nlp_env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dil 1, kapitola 1 scraped successfully.\n",
      "Dil 1, kapitola 2 scraped successfully.\n",
      "Dil 1, kapitola 3 scraped successfully.\n",
      "Dil 1, kapitola 4 scraped successfully.\n",
      "Dil 1, kapitola 5 scraped successfully.\n",
      "Dil 1, kapitola 6 scraped successfully.\n",
      "Server error 502 for https://klempera.tripod.com/svejk07.htm. Retrying...\n",
      "Server error 502 for https://klempera.tripod.com/svejk07.htm. Retrying...\n",
      "Dil 1, kapitola 7 scraped successfully.\n",
      "Dil 1, kapitola 8 scraped successfully.\n",
      "Dil 1, kapitola 9 scraped successfully.\n",
      "Dil 1, kapitola 10 scraped successfully.\n",
      "Dil 1, kapitola 11 scraped successfully.\n",
      "Dil 1, kapitola 12 scraped successfully.\n",
      "Dil 1, kapitola 13 scraped successfully.\n",
      "Dil 1, kapitola 14 scraped successfully.\n",
      "Dil 1, kapitola 15 scraped successfully.\n",
      "Dil 2, kapitola 1 scraped successfully.\n",
      "Dil 2, kapitola 2 scraped successfully.\n",
      "Dil 2, kapitola 3 scraped successfully.\n",
      "Dil 2, kapitola 4 scraped successfully.\n",
      "Dil 2, kapitola 5 scraped successfully.\n",
      "Dil 3, kapitola 1 scraped successfully.\n",
      "Dil 3, kapitola 2 scraped successfully.\n",
      "Dil 3, kapitola 3 scraped successfully.\n",
      "Dil 3, kapitola 4 scraped successfully.\n",
      "Dil 4, kapitola 1 scraped successfully.\n",
      "Dil 4, kapitola 2 scraped successfully.\n",
      "Dil 4, kapitola 3 scraped successfully.\n",
      "All chapters saved to \"svejk_chapters.jsonl\" as JSONL.\n"
     ]
    }
   ],
   "source": [
    "import bs4, requests, json\n",
    "from time import sleep\n",
    "\n",
    "# The novel is in the public domain.\n",
    "# This is where the text will be scraped from. \n",
    "BASE_URL = 'https://klempera.tripod.com/svejk{i:0>2}.htm'\n",
    "\n",
    "# The index 'i' in 'svejk{i}.htm'  needs to be mapped to a pair (part, chapter)\n",
    "MAP_DIL_KAPITOLA = {}\n",
    "# The chapter (kapitola) values of i for each part (dil) 1-4 are as follows:\n",
    "RANGES_DIL_KAPITOLA = [(1, 15), (16, 20), (21, 24), (25, 27)]\n",
    "\n",
    "for part, (first, last) in enumerate(RANGES_DIL_KAPITOLA, start=1):\n",
    "    for i in range(first, last + 1):\n",
    "        MAP_DIL_KAPITOLA[i] = (part, i - first + 1)\n",
    "POOL_i = range(1, 28)\n",
    "\n",
    "ENCODING = 'windows-1250' # Important: the webpage is not in UTF-8.\n",
    "HEADERS= {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} \n",
    "\n",
    "# This is to handle the occasional 502s.\n",
    "MAX_RETRIES = 5\n",
    "WAIT_TIME = 0.5 # Time is in seconds.\n",
    "\n",
    "def extract_plain_text(url):\n",
    "    for _ in range(MAX_RETRIES):\n",
    "        try:     \n",
    "            response = requests.get(url, headers=HEADERS, timeout=50)\n",
    "            response.raise_for_status()\n",
    "            response.encoding = ENCODING\n",
    "        \n",
    "            html_data = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "            all_text = html_data.get_text(separator=' ', strip=True)\n",
    "            return all_text\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print (f'Server error {e.response.status_code} for {url}. Retrying...')\n",
    "            sleep(WAIT_TIME)        \n",
    "\n",
    "BOOK_TITLE = 'Osudy dobrého vojáka Švejka za světové války'\n",
    "AUTHOR = 'Jaroslav Hašek'\n",
    "\n",
    "all_chapters = [] \n",
    "for i in POOL_i:\n",
    "    raw_text = extract_plain_text(BASE_URL.format(i=i))\n",
    "    if raw_text:\n",
    "        clean_text = raw_text.replace('&nbsp', ' ')\n",
    "        part_num, chapter_num_in_part = MAP_DIL_KAPITOLA.get(i, (None, None))\n",
    "\n",
    "        chapter_data = {'book_title': BOOK_TITLE, 'author': AUTHOR,\n",
    "                        'part_number': part_num, 'chapter_number': chapter_num_in_part,\n",
    "                        'chapter_text': clean_text}\n",
    "        all_chapters.append(chapter_data)\n",
    "        print (f'Dil {part_num}, kapitola {chapter_num_in_part} scraped successfully.')\n",
    "    else:\n",
    "        print (f'Could not extract the text. Skipping dil {part_num}, kapitola  {chapter_num_in_part}.')\n",
    "        \n",
    "    sleep(WAIT_TIME) # Being polite.\n",
    "\n",
    "output_jsonl_filename = './data/clean/svejk_chapters.jsonl'\n",
    "try:\n",
    "    with open(output_jsonl_filename, 'w', encoding='utf-8') as f:\n",
    "        for chapter in all_chapters:\n",
    "            f.write(json.dumps(chapter, ensure_ascii=False) + '\\n')\n",
    "    print (f'All chapters saved to \"{output_jsonl_filename}\" as JSONL.')\n",
    "except Exception as e:\n",
    "    print (f'Error saving JSONL: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
