{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a1a30c-be69-4eca-b005-f1fff88f7a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>municipality</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abertamy</th>\n",
       "      <td>['Ostrov']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adamov</th>\n",
       "      <td>['České Budějovice', 'Čáslav', 'Blansko']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adršpach</th>\n",
       "      <td>['Broumov']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albertov</th>\n",
       "      <td>['Praha']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albrechtice</th>\n",
       "      <td>['Havířov', 'Lanškroun']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               district\n",
       "municipality                                           \n",
       "Abertamy                                     ['Ostrov']\n",
       "Adamov        ['České Budějovice', 'Čáslav', 'Blansko']\n",
       "Adršpach                                    ['Broumov']\n",
       "Albertov                                      ['Praha']\n",
       "Albrechtice                    ['Havířov', 'Lanškroun']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the UDPipe library via its Python binding.\n",
    "# Source: http://ufal.mff.cuni.cz/udpipe\n",
    "from ufal.udpipe import Model, Pipeline, InputFormat\n",
    "import ufal.udpipe\n",
    "\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "import ahocorasick\n",
    "import json\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Loading the text.\n",
    "output_all_chapters = './data/clean/svejk_chapters.jsonl'\n",
    "all_chapters = []\n",
    "\n",
    "with open(output_all_chapters, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f): # Each \"line\" is a block of chapter data.\n",
    "        try:\n",
    "            chapter = json.loads(line.strip())\n",
    "            all_chapters.append(chapter)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: Could not decode JSON on line {line_num + 1}: {e}.\")\n",
    "            continue \n",
    "\n",
    "# Loading the locations names base.\n",
    "output_base = './data/clean/geo_base.csv'\n",
    "\n",
    "df_base = pd.read_csv(output_base)\n",
    "df_base.set_index('municipality', inplace=True)\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956ba340-c683-422c-8dae-46fea9dd1ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collisions found: ['Březina', 'Březiny'] -> (('Březina', 'M', False),)\n",
      "Collisions found: ['Džbán', 'Džbánov'] -> (('Džbán', 'M', False),)\n",
      "Collisions found: ['Hlince', 'Hlinka'] -> (('Hlinka', 'M', False),)\n",
      "Collisions found: ['Hradce', 'Hradec'] -> (('Hradec', 'M', False),)\n",
      "Collisions found: ['Luby', 'Lubě'] -> (('Luba', 'F', True),)\n",
      "Collisions found: ['Radkov', 'Radkovy'] -> (('Radkov', 'M', False),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(('nad', None, False), 197),\n",
       "  (('u', None, False), 149),\n",
       "  (('U', None, False), 96),\n",
       "  (('Na', None, False), 60),\n",
       "  (('Lhota', 'F', False), 54),\n",
       "  (('pod', None, False), 53),\n",
       "  (('Horní', 'F', False), 52),\n",
       "  (('Dolní', 'F', False), 40),\n",
       "  (('Horní', 'M', False), 38),\n",
       "  (('Nový', 'F', False), 37)],\n",
       " 5754)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use an older UDPipe 2.5 model for Czech. \n",
    "# See https://ufal.mff.cuni.cz/udpipe/2/models for more recent ones.\n",
    "\n",
    "model = Model.load('czech-pdt-ud-2.5-191206.udpipe') # Source: http://hdl.handle.net/11234/1-3131\n",
    "\n",
    "if model is None:\n",
    "    raise Exception('Cannot load the UDPipe model.')\n",
    "\n",
    "# Bringing up a UDPipe pipeline to tokenize and lemmatize.\n",
    "udpipe_pipeline = Pipeline(model, 'tokenize', 'tag', Pipeline.DEFAULT, Pipeline.DEFAULT) \n",
    "#udpipe_pipeline = Pipeline(udpipe_model, 'tokenize', 'tag', 'parse', 'segment')\n",
    "\n",
    "input_format = InputFormat.newInputFormat('conllu')\n",
    "error = ufal.udpipe.ProcessingError()\n",
    "\n",
    "# Some helper functions.\n",
    "\n",
    "# Maps a word, as an instance of UDPipe.Word, to the triple (lemma, gender, plural flag). \n",
    "def word_to_triple(word):\n",
    "    if word.form in CUSTOM_LEMMA:\n",
    "        return CUSTOM_LEMMA[word.form]\n",
    "\n",
    "    # Keep capitalization. This helps to reduce (but not to eliminate entirely) false matches for the settlement names\n",
    "    # that consist of a single adjective (e.g. Krásno, Krásna, Dobrá, Dobré). \n",
    "    lemma = word.lemma\n",
    "    if word.form.istitle():\n",
    "        lemma = lemma.title()\n",
    "        \n",
    "    pos = word.feats.find('Gender=')\n",
    "    gender = word.feats[pos + 7] if pos >= 0 else None # 7 == len('Gender=')\n",
    "\n",
    "    return (lemma, gender, 'Number=Plur' in word.feats)\n",
    "\n",
    "# Map each word in a sentence to the triple (lemma, gender, plural flag). \n",
    "# The argument can be a string or an instance of UDPipe.Sentence.\n",
    "def lemmatize(sent):\n",
    "    if isinstance(sent, str):\n",
    "        processed_sent = udpipe_pipeline.process(sent)\n",
    "        input_format.setText(processed_sent)\n",
    "\n",
    "        sentence = ufal.udpipe.Sentence()\n",
    "        res = []\n",
    "        # Sometimes redundant sentence breaks appear, even when 'sent' is a single sentence (which is the indendent usage case).\n",
    "        while input_format.nextSentence(sentence, error):\n",
    "            model.tag(sentence, model.DEFAULT)\n",
    "            model.parse(sentence, model.DEFAULT)\n",
    "            res.extend(word_to_triple(word) for word in sentence.words if word.lemma[0].isalpha())\n",
    "            sentence = ufal.udpipe.Sentence()\n",
    "        if error.occurred():\n",
    "            raise Exception(f'Error while reading a sentence: {error.message}')  \n",
    "        return tuple(res)\n",
    "\n",
    "    elif isinstance(sent, ufal.udpipe.Sentence):\n",
    "        sentence = sent\n",
    "    else:\n",
    "        raise Exception(f'Cannot proceed with an argument of type {type(sent)}.')\n",
    "        \n",
    "    model.tag(sentence, model.DEFAULT)\n",
    "    #model.parse(sentence, model.DEFAULT)\n",
    "    \n",
    "    res = [word_to_triple(word) for word in sentence.words if word.lemma[0].isalpha()]\n",
    "\n",
    "    return tuple(res)\n",
    "\n",
    "# Map a triple (lemma, gender, plural flag) to a string.\n",
    "def compress(triple):\n",
    "    lemma, gender, plural_flag = triple\n",
    "    if gender:\n",
    "        return lemma + gender + str(int(plural_flag)) + '#'\n",
    "    return lemma + '#'\n",
    "\n",
    "# Given a list of tuples ((a, b), <data>), remove all such tuples where (a,b) is contained, as an interval, in (c,d)\n",
    "# for some other ((c,d), <data>) in arr. This is used for greedy-matching.\n",
    "def remove_inclusions(arr):\n",
    "    res = [] \n",
    "\n",
    "    arr.sort(key=lambda x: x[0][1] - x[0][0], reverse=True)\n",
    "    arr.sort()    \n",
    "    cur_right = -1    \n",
    "    for t in arr:\n",
    "        (_, right), _ = t\n",
    "        if right > cur_right:\n",
    "            res.append(t)\n",
    "            cur_right = right\n",
    "            \n",
    "    return res    \n",
    "\n",
    "# Hard-coding some edge cases.\n",
    "CUSTOM_LEMMA = {'Čím':      ('Čím', 'M', False), # There is a village named Čím (avoid lemmatizing it to 'co').\n",
    "                'Ludvíkov': ('Ludvíkov', 'M', False),\n",
    "                'Zlivi':    ('Zliv', 'M', False)} \n",
    "\n",
    "# Ignore false matches. What constitutes a \"false match\" is task- and context-dependent.\n",
    "# In our case we have to take into account that there are Czech municipalities named \n",
    "# Vídeň, Uhersko, Srbsko, Srby etc. that trigger multiple matches in \"Osudy...\", \n",
    "# while not being the actual locations the author refers to (most likely). \n",
    "# A similar issue comes up for some personal names, nouns and adjectives.\n",
    "IGNORE = {'Vídeň', 'Uhersko', 'Srbsko', 'Srby', 'Rusín', 'Čechy',\n",
    "          'Dub', 'Jaroslav', 'Karlík', 'Ludvík', 'Slavíkov', \n",
    "          'Hosty', 'Kámen',\n",
    "          'Bílá', 'Bystrá', 'Černá', 'Dobré', 'Dobrá', 'Milý', 'Svatá', \n",
    "          'Čím'} # All are actual localities.\n",
    "\n",
    "ALIAS = {'Budějovice' : 'České Budějovice', # A choice made at the expense of missing possible references to Moravské Budějovice.\n",
    "         'Bojiště': 'Praha', # Ulice Na Bojišti (in Prague) is a fairly important locality in the novel. \n",
    "                             # There are several other localities named Bojiště though.\n",
    "         'Kralupy': 'Kralupy nad Vltavou'}\n",
    "\n",
    "name_to_lemmata = {}\n",
    "lemmata_to_name = defaultdict(lambda: [])\n",
    "for name in df_base.index:\n",
    "    lemmata = lemmatize(name)\n",
    "    name_to_lemmata[name] = lemmata \n",
    "    lemmata_to_name[lemmata].append(name)\n",
    "\n",
    "for lemmata, name in lemmata_to_name.items():\n",
    "    if len(name) > 1:\n",
    "        print (f'Collisions found: {name} -> {lemmata}')\n",
    "\n",
    "for name in ALIAS:\n",
    "    lemmata = lemmatize(name)\n",
    "    name_to_lemmata[name] = lemmata\n",
    "    lemmata_to_name[lemmata].append(ALIAS[name])\n",
    "\n",
    "# The most common lemmata in our toponyms base.\n",
    "unique_count = Counter(lemma for lemmata in name_to_lemmata.values() for lemma in lemmata)\n",
    "unique_count.most_common(10), len(unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8ffed88-51a0-487c-be77-b1e70f4cb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST1 = 'Kostel svatého Petra a Pavla v Kostomlatech pod Řípem v okrese Litoměřice je původně filiální římskokatolický kostel.'\n",
    "# TEST2 = 'Jiřice u Moravských Budějovic je obec v České republice, malebně polożená na okraji přírodního parku Jevišovka. '\n",
    "# TEST3 = 'Jednou v Mydlovarech u Zlivi, okres Hluboká, okresní hejtmanství České Budějovice, právě když jsme tam měli jednadevadesátí cvičení.'\n",
    "\n",
    "# print (lemmatize(TEST1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a79b07-e9bf-4820-af8b-971b0853e47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 chapter(s) processed. 5 location references identified.\n",
      "2 chapter(s) processed. 8 location references identified.\n",
      "3 chapter(s) processed. 10 location references identified.\n",
      "4 chapter(s) processed. 12 location references identified.\n",
      "5 chapter(s) processed. 18 location references identified.\n",
      "6 chapter(s) processed. 21 location references identified.\n",
      "7 chapter(s) processed. 24 location references identified.\n",
      "8 chapter(s) processed. 28 location references identified.\n",
      "9 chapter(s) processed. 31 location references identified.\n",
      "10 chapter(s) processed. 49 location references identified.\n",
      "11 chapter(s) processed. 57 location references identified.\n",
      "12 chapter(s) processed. 60 location references identified.\n",
      "13 chapter(s) processed. 65 location references identified.\n",
      "14 chapter(s) processed. 95 location references identified.\n",
      "15 chapter(s) processed. 101 location references identified.\n",
      "16 chapter(s) processed. 129 location references identified.\n",
      "17 chapter(s) processed. 273 location references identified.\n",
      "18 chapter(s) processed. 306 location references identified.\n",
      "19 chapter(s) processed. 315 location references identified.\n",
      "20 chapter(s) processed. 340 location references identified.\n"
     ]
    }
   ],
   "source": [
    "# Bringing up an Aho-Corasick automaton. The patterns to search for in the text are the lemmata patterns of the toponyms.\n",
    "A = ahocorasick.Automaton()\n",
    "index_to_lemmata = {}\n",
    "length = {}\n",
    "for index, lemmata in enumerate(lemmata_to_name.keys()):\n",
    "    lemmata_compressed = ''.join(compress(triple) for triple in lemmata)\n",
    "    A.add_word(lemmata_compressed, index)\n",
    "    index_to_lemmata[index] = lemmata\n",
    "    length[index] = len(lemmata_compressed)\n",
    "A.make_automaton()\n",
    "\n",
    "all_results = []\n",
    "reference_counter = Counter()\n",
    "\n",
    "# Iterating over chapters.\n",
    "for chapter in all_chapters:\n",
    "    chapter_text = chapter.get('chapter_text')\n",
    "    \n",
    "    # This is a container to hold the references found in a current chapter.    \n",
    "    chapter_results = {key: value for key, value in chapter.items() if key != 'chapter_text'}\n",
    "    chapter_results['sentence_matches'] = []\n",
    "    \n",
    "    processed_text = udpipe_pipeline.process(chapter_text)\n",
    "    input_format.setText(processed_text)\n",
    "\n",
    "    # Iterating over sentences.\n",
    "    sentence = ufal.udpipe.Sentence()\n",
    "    sentence_cnt = 0\n",
    "    while input_format.nextSentence(sentence, error):\n",
    "        sentence_cnt += 1\n",
    "        compressed_sentence = ''.join(compress(triple) for triple in lemmatize(sentence))\n",
    "        found_matches = [((end_pos - length[index] + 1, end_pos), index) for end_pos, index in A.iter(compressed_sentence)]\n",
    "        found_matches = remove_inclusions(found_matches)        \n",
    "        if found_matches:\n",
    "            chapter_results['sentence_matches'].append({'sentence_id': sentence_cnt, \n",
    "                                                        'sentence_text': sentence.getText(),\n",
    "                                                        'matches': []})        \n",
    "            for _, index in found_matches:\n",
    "                for name_found in lemmata_to_name[index_to_lemmata[index]]:\n",
    "                    if name_found not in IGNORE:\n",
    "                        chapter_results['sentence_matches'][-1]['matches'].append({'location:': name_found,\n",
    "                                                                                   'metadata' : df_base.loc[name_found, 'district']}) \n",
    "                        reference_counter[name_found] += 1\n",
    "                        \n",
    "            # If there are no matches or all matches had to be ignored, do not include the sentence into the report.\n",
    "            if not chapter_results['sentence_matches'][-1]['matches']:\n",
    "                chapter_results['sentence_matches'].pop()                \n",
    "        #sentence = ufal.udpipe.Sentence()\n",
    "\n",
    "    if error.occurred():\n",
    "        raise Exception(f'An error while processing sentence {sentence_cnt}: {error.message}')\n",
    "        \n",
    "    all_results.append(chapter_results)\n",
    "    print (f'{len(all_results)} chapter(s) processed. {reference_counter.total()} location references identified.')\n",
    "\n",
    "output_jsonl_filename = './data/clean/svejk_geo_references.jsonl'\n",
    "try:\n",
    "    with open(output_jsonl_filename, 'w', encoding='utf-8') as f:\n",
    "        for chapter_results in all_results:\n",
    "            f.write(json.dumps(chapter_results, ensure_ascii=False) + '\\n')\n",
    "    print (f'The results are saved to \"{output_jsonl_filename}\" as JSONL.')\n",
    "except Exception as e:\n",
    "    print (f'Error saving JSONL: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2be9f5-6d36-4fe1-a11e-14247e3f9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reference_counter, len(reference_counter))\n",
    "\n",
    "n = 15\n",
    "top_n = pd.Series(dict(reference_counter.most_common(n)))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "top_n.plot(kind='barh')\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5) \n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426074d9-d552-43c0-a8c4-c62cd54bef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    collocations=False,\n",
    "    max_words=120,\n",
    "    min_font_size=10,\n",
    "    colormap='viridis'\n",
    ")\n",
    "\n",
    "wordcloud.generate_from_frequencies(reference_counter)\n",
    "\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.imshow(wordcloud, interpolation='bilinear') \n",
    "plt.axis('off')\n",
    "\n",
    "plt.savefig('svejk-geo-wordcloud.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a14c9-3cb0-4efe-a036-fa41b6c1545b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
